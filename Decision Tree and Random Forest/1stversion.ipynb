{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Melbourne housing data\n",
    "melbourne_file_path = 'C:/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision Tree and Random Forest/melb_data.csv'\n",
    "data = pd.read_csv(melbourne_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target (y) and predictors (X)\n",
    "y = data.Price\n",
    "predictors = data.drop(['Price'], axis=1)\n",
    "\n",
    "# Split predictors into numerical and categorical features\n",
    "X_numerical = predictors.select_dtypes(exclude=['object'])\n",
    "X_categorical = predictors.select_dtypes('object')\n",
    "\n",
    "# Identify categorical columns\n",
    "variables = (X_categorical.dtypes == 'object')\n",
    "object_cols = list(variables[variables].index)\n",
    "\n",
    "# Calculate cardinality for each categorical variable\n",
    "var_card = []\n",
    "for variable in object_cols:\n",
    "    unique_values = X_categorical[variable].unique()\n",
    "    cardinality = len(unique_values)\n",
    "    var_card.append(cardinality)\n",
    "\n",
    "# Create a DataFrame to display variable cardinality\n",
    "df = pd.DataFrame({'Variable': [var for var in object_cols], 'Cardinality': [car for car in var_card]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Suburb_freq  Address_freq  SellerG_freq  Date_freq  CouncilArea_freq  \\\n",
      "0         0.004124      0.000074      0.028940   0.021208          0.052985   \n",
      "1         0.004124      0.000074      0.028940   0.001915          0.052985   \n",
      "2         0.004124      0.000221      0.028940   0.024816          0.052985   \n",
      "3         0.004124      0.000074      0.028940   0.024816          0.052985   \n",
      "4         0.004124      0.000074      0.115243   0.015758          0.052985   \n",
      "...            ...           ...           ...        ...               ...   \n",
      "13575     0.001915      0.000074      0.074448   0.018115               NaN   \n",
      "13576     0.007879      0.000074      0.008174   0.018115               NaN   \n",
      "13577     0.007879      0.000074      0.008542   0.018115               NaN   \n",
      "13578     0.007879      0.000074      0.015906   0.018115               NaN   \n",
      "13579     0.012077      0.000074      0.009205   0.018115               NaN   \n",
      "\n",
      "         0    1    2    3    4  ...  Postcode  Bedroom2  Bathroom  Car  \\\n",
      "0      1.0  0.0  0.0  0.0  1.0  ...    3067.0       2.0       1.0  1.0   \n",
      "1      1.0  0.0  0.0  0.0  1.0  ...    3067.0       2.0       1.0  0.0   \n",
      "2      1.0  0.0  0.0  0.0  0.0  ...    3067.0       3.0       2.0  0.0   \n",
      "3      1.0  0.0  0.0  1.0  0.0  ...    3067.0       3.0       2.0  1.0   \n",
      "4      1.0  0.0  0.0  0.0  0.0  ...    3067.0       3.0       1.0  2.0   \n",
      "...    ...  ...  ...  ...  ...  ...       ...       ...       ...  ...   \n",
      "13575  1.0  0.0  0.0  0.0  1.0  ...    3150.0       4.0       2.0  2.0   \n",
      "13576  1.0  0.0  0.0  0.0  0.0  ...    3016.0       3.0       2.0  2.0   \n",
      "13577  1.0  0.0  0.0  0.0  1.0  ...    3016.0       3.0       2.0  4.0   \n",
      "13578  1.0  0.0  0.0  1.0  0.0  ...    3016.0       4.0       1.0  5.0   \n",
      "13579  1.0  0.0  0.0  0.0  0.0  ...    3013.0       4.0       1.0  1.0   \n",
      "\n",
      "       Landsize  BuildingArea  YearBuilt  Lattitude  Longtitude  Propertycount  \n",
      "0         202.0           NaN        NaN  -37.79960   144.99840         4019.0  \n",
      "1         156.0          79.0     1900.0  -37.80790   144.99340         4019.0  \n",
      "2         134.0         150.0     1900.0  -37.80930   144.99440         4019.0  \n",
      "3          94.0           NaN        NaN  -37.79690   144.99690         4019.0  \n",
      "4         120.0         142.0     2014.0  -37.80720   144.99410         4019.0  \n",
      "...         ...           ...        ...        ...         ...            ...  \n",
      "13575     652.0           NaN     1981.0  -37.90562   145.16761         7392.0  \n",
      "13576     333.0         133.0     1995.0  -37.85927   144.87904         6380.0  \n",
      "13577     436.0           NaN     1997.0  -37.85274   144.88738         6380.0  \n",
      "13578     866.0         157.0     1920.0  -37.85908   144.89299         6380.0  \n",
      "13579     362.0         112.0     1920.0  -37.81188   144.88449         6543.0  \n",
      "\n",
      "[13580 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define columns to one-hot encode\n",
    "categorical_cols_to_one_hot_encode = ['Type', 'Method', 'Regionname']\n",
    "X_categorical_to_encode = X_categorical[categorical_cols_to_one_hot_encode]\n",
    "\n",
    "# One-hot encode the selected categorical columns\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols = pd.DataFrame(OH_encoder.fit_transform(X_categorical_to_encode))\n",
    "OH_cols.index = X_categorical_to_encode.index\n",
    "OH_cols.columns = OH_cols.columns.astype(str)\n",
    "\n",
    "# Drop original categorical columns that were one-hot encoded\n",
    "X_categorical = X_categorical.drop(categorical_cols_to_one_hot_encode, axis=1)\n",
    "\n",
    "# Concatenate one-hot encoded columns with the original categorical columns\n",
    "X_categorical_encoded = pd.concat([X_categorical, OH_cols], axis=1)\n",
    "X_categorical_encoded.columns = X_categorical_encoded.columns.astype(str)\n",
    "\n",
    "# Define columns to perform frequency encoding\n",
    "categorical_cols_to_frequency_encode = ['Suburb', 'Address', 'SellerG', 'Date', 'CouncilArea']\n",
    "\n",
    "# Perform frequency encoding on selected categorical columns\n",
    "for col in categorical_cols_to_frequency_encode:\n",
    "    freq = X_categorical[col].value_counts(normalize=True)\n",
    "    X_categorical[col+'_freq'] = X_categorical[col].map(freq)\n",
    "\n",
    "# Drop original categorical columns used for frequency encoding\n",
    "X_categorical = X_categorical.drop(categorical_cols_to_frequency_encode, axis=1)\n",
    "\n",
    "# Concatenate one-hot encoded columns with the original categorical columns\n",
    "X_categorical_encoded = pd.concat([X_categorical, OH_cols], axis=1)\n",
    "\n",
    "# Concatenate one-hot encoded and numerical features\n",
    "X = pd.concat([X_categorical_encoded, X_numerical], axis=1)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each column\n",
    "missing_val_count_by_column = (X.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Identify columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "# Make copies to avoid changing the original data during imputation\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Create new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Impute missing values\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate Mean Absolute Error (MAE) for Random Forest Regressor\n",
    "def get_mae_rf(n_estimators, imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, random_state=0)\n",
    "    model.fit(imputed_X_train_plus, y_train)\n",
    "    preds_val = model.predict(imputed_X_valid_plus)\n",
    "    mae = mean_absolute_error(y_valid, preds_val)\n",
    "    return mae\n",
    "\n",
    "# Test different values of n_estimators and print their MAE\n",
    "print(\"Testing different n_estimators:\")\n",
    "estimators, mae_rf = [], []\n",
    "number_of_estimators = [10, 50, 100, 200]\n",
    "print(\"Running tests for optimal number of estimators\")\n",
    "for n_estimators in number_of_estimators:\n",
    "    my_mae = get_mae_rf(n_estimators, imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid)\n",
    "    estimators.append(n_estimators)\n",
    "    #print(\"n_estimators: %d  \\t\\t Mean Absolute Error: %d\" % (n_estimators, my_mae))\n",
    "    mae_rf.append(my_mae)\n",
    "\n",
    "# Create a dictionary to store n_estimators as keys and their corresponding MAEs as values\n",
    "pairs = {estimators[i]: mae_rf[i] for i in range(len(estimators))}\n",
    "\n",
    "# Find the key with the lowest value (i.e., the best n_estimators value) using min() and a custom key function\n",
    "optimal_n_estimators = min(pairs, key=pairs.get)\n",
    "print(\"Optimal n_estimators:\", optimal_n_estimators)\n",
    "\n",
    "# Train the model and make predictions\n",
    "model = RandomForestRegressor(n_estimators = optimal_n_estimators, random_state=0)\n",
    "model.fit(imputed_X_train_plus, y_train)\n",
    "randomForestPredictions = model.predict(imputed_X_valid_plus)\n",
    "mae_rf = mean_absolute_error(y_valid, randomForestPredictions)\n",
    "print(f\"Random Forest MAE: {mae_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate Mean Absolute Error (MAE) for Decision Tree Regressor\n",
    "def get_mae_dt(max_leaf_nodes, imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(imputed_X_train_plus, y_train)\n",
    "    preds_val = model.predict(imputed_X_valid_plus)\n",
    "    mae = mean_absolute_error(y_valid, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "# Test different values of max_leaf_nodes and print their MAE\n",
    "print(\"Testing different max_leaf_nodes:\")\n",
    "nodes, mae_dt = [], []\n",
    "number_of_leaf_nodes = [5, 50, 500, 5000]\n",
    "for max_leaf_nodes in number_of_leaf_nodes:\n",
    "    my_mae = get_mae_dt(max_leaf_nodes, imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid)\n",
    "    nodes.append(max_leaf_nodes)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "    mae_dt.append(my_mae)\n",
    "\n",
    "# Create a dictionary to store max_leaf_nodes as keys and their corresponding MAEs as values\n",
    "pairs = {nodes[i]: mae_dt[i] for i in range(len(nodes))}\n",
    "\n",
    "# Find the key with the lowest value (i.e., the best max_leaf_nodes value) using min() and a custom key function\n",
    "key_with_lowest_mae = min(pairs, key=pairs.get)\n",
    "print(\"Key with lowest MAE:\", key_with_lowest_mae)\n",
    "\n",
    "# Extract keys and values from the dictionary\n",
    "keys, values = list(pairs.keys()), list(pairs.values())\n",
    "\n",
    "# Decision Tree \n",
    "melbourne_model = DecisionTreeRegressor(max_leaf_nodes=key_with_lowest_mae, random_state=1)\n",
    "melbourne_model.fit(imputed_X_train_plus, y_train)\n",
    "decisionTreePredictions = melbourne_model.predict(imputed_X_valid_plus)\n",
    "mae_dt = mean_absolute_error(y_valid, decisionTreePredictions)\n",
    "print(f\"Decision Tree MAE: {mae_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CouncilArea_freq    1369\n",
      "Car                   62\n",
      "BuildingArea        6450\n",
      "YearBuilt           5375\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different n_estimators:\n",
      "Running tests for optimal number of estimators\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\marti\\Desktop\\projects\\HousePricePredictor-Kaggle\\Decision Tree and Random Forest\\1stversion.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision%20Tree%20and%20Random%20Forest/1stversion.ipynb#W4sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning tests for optimal number of estimators\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision%20Tree%20and%20Random%20Forest/1stversion.ipynb#W4sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m n_estimators \u001b[39min\u001b[39;00m number_of_estimators:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision%20Tree%20and%20Random%20Forest/1stversion.ipynb#W4sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     my_mae \u001b[39m=\u001b[39m get_mae_rf(n_estimators, imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision%20Tree%20and%20Random%20Forest/1stversion.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     estimators\u001b[39m.\u001b[39mappend(n_estimators)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision%20Tree%20and%20Random%20Forest/1stversion.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39m#print(\"n_estimators: %d  \\t\\t Mean Absolute Error: %d\" % (n_estimators, my_mae))\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\marti\\Desktop\\projects\\HousePricePredictor-Kaggle\\Decision Tree and Random Forest\\1stversion.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision%20Tree%20and%20Random%20Forest/1stversion.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_mae_rf\u001b[39m(n_estimators, imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision%20Tree%20and%20Random%20Forest/1stversion.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     model \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39mn_estimators, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision%20Tree%20and%20Random%20Forest/1stversion.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(imputed_X_train_plus, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision%20Tree%20and%20Random%20Forest/1stversion.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     preds_val \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(imputed_X_valid_plus)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision%20Tree%20and%20Random%20Forest/1stversion.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     mae \u001b[39m=\u001b[39m mean_absolute_error(y_valid, preds_val)\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to compare actual vs. predicted results\n",
    "comparison_df = pd.DataFrame({'Actual': y_valid,\n",
    "                               'RandomForestPredictions': randomForestPredictions,\n",
    "                               'DecisionTreePrediction': decisionTreePredictions})\n",
    "comparison_df['RandomForestPredictions'] = comparison_df['RandomForestPredictions'].round(2)\n",
    "comparison_df['DecisionTreePrediction'] = comparison_df['DecisionTreePrediction'].round(2)\n",
    "\n",
    "# Present the model and the results\n",
    "print(\"Comparison of Actual vs. Predicted Results:\\n\", comparison_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
