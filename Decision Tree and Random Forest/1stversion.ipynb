{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Melbourne housing data\n",
    "melbourne_file_path = 'C:/Users/marti/Desktop/projects/HousePricePredictor-Kaggle/Decision Tree and Random Forest/melb_data.csv'\n",
    "data = pd.read_csv(melbourne_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target (y) and predictors (X)\n",
    "y = data.Price\n",
    "predictors = data.drop(['Price'], axis=1)\n",
    "\n",
    "# Split predictors into numerical and categorical features\n",
    "X_numerical = predictors.select_dtypes(exclude=['object'])\n",
    "X_categorical = predictors.select_dtypes('object')\n",
    "\n",
    "# Identify categorical columns\n",
    "variables = (X_categorical.dtypes == 'object')\n",
    "object_cols = list(variables[variables].index)\n",
    "\n",
    "# Calculate cardinality for each categorical variable\n",
    "var_card = []\n",
    "for variable in object_cols:\n",
    "    unique_values = X_categorical[variable].unique()\n",
    "    cardinality = len(unique_values)\n",
    "    var_card.append(cardinality)\n",
    "\n",
    "# Create a DataFrame to display variable cardinality\n",
    "df = pd.DataFrame({'Variable': [var for var in object_cols], 'Cardinality': [car for car in var_card]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Suburb_freq  Address_freq  SellerG_freq  Date_freq  CouncilArea_freq  \\\n",
      "0         0.004124      0.000074      0.028940   0.021208          0.052985   \n",
      "1         0.004124      0.000074      0.028940   0.001915          0.052985   \n",
      "2         0.004124      0.000221      0.028940   0.024816          0.052985   \n",
      "3         0.004124      0.000074      0.028940   0.024816          0.052985   \n",
      "4         0.004124      0.000074      0.115243   0.015758          0.052985   \n",
      "...            ...           ...           ...        ...               ...   \n",
      "13575     0.001915      0.000074      0.074448   0.018115               NaN   \n",
      "13576     0.007879      0.000074      0.008174   0.018115               NaN   \n",
      "13577     0.007879      0.000074      0.008542   0.018115               NaN   \n",
      "13578     0.007879      0.000074      0.015906   0.018115               NaN   \n",
      "13579     0.012077      0.000074      0.009205   0.018115               NaN   \n",
      "\n",
      "         0    1    2    3    4  ...  Postcode  Bedroom2  Bathroom  Car  \\\n",
      "0      1.0  0.0  0.0  0.0  1.0  ...    3067.0       2.0       1.0  1.0   \n",
      "1      1.0  0.0  0.0  0.0  1.0  ...    3067.0       2.0       1.0  0.0   \n",
      "2      1.0  0.0  0.0  0.0  0.0  ...    3067.0       3.0       2.0  0.0   \n",
      "3      1.0  0.0  0.0  1.0  0.0  ...    3067.0       3.0       2.0  1.0   \n",
      "4      1.0  0.0  0.0  0.0  0.0  ...    3067.0       3.0       1.0  2.0   \n",
      "...    ...  ...  ...  ...  ...  ...       ...       ...       ...  ...   \n",
      "13575  1.0  0.0  0.0  0.0  1.0  ...    3150.0       4.0       2.0  2.0   \n",
      "13576  1.0  0.0  0.0  0.0  0.0  ...    3016.0       3.0       2.0  2.0   \n",
      "13577  1.0  0.0  0.0  0.0  1.0  ...    3016.0       3.0       2.0  4.0   \n",
      "13578  1.0  0.0  0.0  1.0  0.0  ...    3016.0       4.0       1.0  5.0   \n",
      "13579  1.0  0.0  0.0  0.0  0.0  ...    3013.0       4.0       1.0  1.0   \n",
      "\n",
      "       Landsize  BuildingArea  YearBuilt  Lattitude  Longtitude  Propertycount  \n",
      "0         202.0           NaN        NaN  -37.79960   144.99840         4019.0  \n",
      "1         156.0          79.0     1900.0  -37.80790   144.99340         4019.0  \n",
      "2         134.0         150.0     1900.0  -37.80930   144.99440         4019.0  \n",
      "3          94.0           NaN        NaN  -37.79690   144.99690         4019.0  \n",
      "4         120.0         142.0     2014.0  -37.80720   144.99410         4019.0  \n",
      "...         ...           ...        ...        ...         ...            ...  \n",
      "13575     652.0           NaN     1981.0  -37.90562   145.16761         7392.0  \n",
      "13576     333.0         133.0     1995.0  -37.85927   144.87904         6380.0  \n",
      "13577     436.0           NaN     1997.0  -37.85274   144.88738         6380.0  \n",
      "13578     866.0         157.0     1920.0  -37.85908   144.89299         6380.0  \n",
      "13579     362.0         112.0     1920.0  -37.81188   144.88449         6543.0  \n",
      "\n",
      "[13580 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define columns to one-hot encode\n",
    "categorical_cols_to_one_hot_encode = ['Type', 'Method', 'Regionname']\n",
    "X_categorical_to_encode = X_categorical[categorical_cols_to_one_hot_encode]\n",
    "\n",
    "# One-hot encode the selected categorical columns\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols = pd.DataFrame(OH_encoder.fit_transform(X_categorical_to_encode))\n",
    "OH_cols.index = X_categorical_to_encode.index\n",
    "OH_cols.columns = OH_cols.columns.astype(str)\n",
    "\n",
    "# Drop original categorical columns that were one-hot encoded\n",
    "X_categorical = X_categorical.drop(categorical_cols_to_one_hot_encode, axis=1)\n",
    "\n",
    "# Concatenate one-hot encoded columns with the original categorical columns\n",
    "X_categorical_encoded = pd.concat([X_categorical, OH_cols], axis=1)\n",
    "X_categorical_encoded.columns = X_categorical_encoded.columns.astype(str)\n",
    "\n",
    "# Define columns to perform frequency encoding\n",
    "categorical_cols_to_frequency_encode = ['Suburb', 'Address', 'SellerG', 'Date', 'CouncilArea']\n",
    "\n",
    "# Perform frequency encoding on selected categorical columns\n",
    "for col in categorical_cols_to_frequency_encode:\n",
    "    freq = X_categorical[col].value_counts(normalize=True)\n",
    "    X_categorical[col+'_freq'] = X_categorical[col].map(freq)\n",
    "\n",
    "# Drop original categorical columns used for frequency encoding\n",
    "X_categorical = X_categorical.drop(categorical_cols_to_frequency_encode, axis=1)\n",
    "\n",
    "# Concatenate one-hot encoded columns with the original categorical columns\n",
    "X_categorical_encoded = pd.concat([X_categorical, OH_cols], axis=1)\n",
    "\n",
    "# Concatenate one-hot encoded and numerical features\n",
    "X = pd.concat([X_categorical_encoded, X_numerical], axis=1)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CouncilArea_freq    1369\n",
      "Car                   62\n",
      "BuildingArea        6450\n",
      "YearBuilt           5375\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values in each column\n",
    "missing_val_count_by_column = (X.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Identify columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "# Make copies to avoid changing the original data during imputation\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Create new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Impute missing values\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different n_estimators:\n",
      "Running tests for optimal number of estimators\n",
      "Optimal n_estimators: 200\n",
      "Random Forest MAE: 161224.1028111193\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate Mean Absolute Error (MAE) for Random Forest Regressor\n",
    "def get_mae_rf(n_estimators, imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, random_state=0)\n",
    "    model.fit(imputed_X_train_plus, y_train)\n",
    "    preds_val = model.predict(imputed_X_valid_plus)\n",
    "    mae = mean_absolute_error(y_valid, preds_val)\n",
    "    return mae\n",
    "\n",
    "# Test different values of n_estimators and print their MAE\n",
    "print(\"Testing different n_estimators:\")\n",
    "estimators, mae_rf = [], []\n",
    "number_of_estimators = [10, 50, 100, 200]\n",
    "print(\"Running tests for optimal number of estimators\")\n",
    "for n_estimators in number_of_estimators:\n",
    "    my_mae = get_mae_rf(n_estimators, imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid)\n",
    "    estimators.append(n_estimators)\n",
    "    #print(\"n_estimators: %d  \\t\\t Mean Absolute Error: %d\" % (n_estimators, my_mae))\n",
    "    mae_rf.append(my_mae)\n",
    "\n",
    "# Create a dictionary to store n_estimators as keys and their corresponding MAEs as values\n",
    "pairs = {estimators[i]: mae_rf[i] for i in range(len(estimators))}\n",
    "\n",
    "# Find the key with the lowest value (i.e., the best n_estimators value) using min() and a custom key function\n",
    "optimal_n_estimators = min(pairs, key=pairs.get)\n",
    "print(\"Optimal n_estimators:\", optimal_n_estimators)\n",
    "\n",
    "# Train the model and make predictions\n",
    "model = RandomForestRegressor(n_estimators = optimal_n_estimators, random_state=0)\n",
    "model.fit(imputed_X_train_plus, y_train)\n",
    "randomForestPredictions = model.predict(imputed_X_valid_plus)\n",
    "mae_rf = mean_absolute_error(y_valid, randomForestPredictions)\n",
    "print(f\"Random Forest MAE: {mae_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different max_leaf_nodes:\n",
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  317464\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  235800\n",
      "Max leaf nodes: 500  \t\t Mean Absolute Error:  212286\n",
      "Max leaf nodes: 5000  \t\t Mean Absolute Error:  223520\n",
      "Key with lowest MAE: 500\n",
      "Decision Tree MAE: 214307.91796554727\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate Mean Absolute Error (MAE) for Decision Tree Regressor\n",
    "def get_mae_dt(max_leaf_nodes, imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(imputed_X_train_plus, y_train)\n",
    "    preds_val = model.predict(imputed_X_valid_plus)\n",
    "    mae = mean_absolute_error(y_valid, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "# Test different values of max_leaf_nodes and print their MAE\n",
    "print(\"Testing different max_leaf_nodes:\")\n",
    "nodes, mae_dt = [], []\n",
    "number_of_leaf_nodes = [5, 50, 500, 5000]\n",
    "for max_leaf_nodes in number_of_leaf_nodes:\n",
    "    my_mae = get_mae_dt(max_leaf_nodes, imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid)\n",
    "    nodes.append(max_leaf_nodes)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "    mae_dt.append(my_mae)\n",
    "\n",
    "# Create a dictionary to store max_leaf_nodes as keys and their corresponding MAEs as values\n",
    "pairs = {nodes[i]: mae_dt[i] for i in range(len(nodes))}\n",
    "\n",
    "# Find the key with the lowest value (i.e., the best max_leaf_nodes value) using min() and a custom key function\n",
    "key_with_lowest_mae = min(pairs, key=pairs.get)\n",
    "print(\"Key with lowest MAE:\", key_with_lowest_mae)\n",
    "\n",
    "# Extract keys and values from the dictionary\n",
    "keys, values = list(pairs.keys()), list(pairs.values())\n",
    "\n",
    "# Decision Tree \n",
    "melbourne_model = DecisionTreeRegressor(max_leaf_nodes=key_with_lowest_mae, random_state=1)\n",
    "melbourne_model.fit(imputed_X_train_plus, y_train)\n",
    "decisionTreePredictions = melbourne_model.predict(imputed_X_valid_plus)\n",
    "mae_dt = mean_absolute_error(y_valid, decisionTreePredictions)\n",
    "print(f\"Decision Tree MAE: {mae_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Actual vs. Predicted Results:\n",
      "           Actual  RandomForestPredictions  DecisionTreePrediction\n",
      "8505   2165000.0               1642161.74              1329486.84\n",
      "5523    815000.0                834666.00               865136.21\n",
      "12852   610000.0                620246.50               733335.71\n",
      "4818   1245000.0               1193513.00              1065333.33\n",
      "12812  1160000.0                914320.00               985705.88\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to compare actual vs. predicted results\n",
    "comparison_df = pd.DataFrame({'Actual': y_valid,\n",
    "                               'RandomForestPredictions': randomForestPredictions,\n",
    "                               'DecisionTreePrediction': decisionTreePredictions})\n",
    "comparison_df['RandomForestPredictions'] = comparison_df['RandomForestPredictions'].round(2)\n",
    "comparison_df['DecisionTreePrediction'] = comparison_df['DecisionTreePrediction'].round(2)\n",
    "\n",
    "# Present the model and the results\n",
    "print(\"Comparison of Actual vs. Predicted Results:\\n\", comparison_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
