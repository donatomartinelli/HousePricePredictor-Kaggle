{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# Setting display options for pandas\n",
    "#pd.set_option('display.max_columns', 200)  # Setting the maximum number of displayed columns to 200\n",
    "pd.set_option('display.max_rows', 200)     # Setting the maximum number of displayed rows to 200\n",
    "\n",
    "# Importing warnings module and setting to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def dataset_summary(datasets, display_columns, display_dtype, display_statistics):\n",
    "    \"\"\"\n",
    "    This function summarizes a dataset by providing essential information such as dataset shape,\n",
    "    total cells, missing data statistics, columns with missing values, and data type counts.\n",
    "    \n",
    "    Parameters:\n",
    "    datasets (DataFrame): The dataset to be summarized.\n",
    "    display_columns (str): A flag indicating whether to display dataset columns or not (\"y\" for yes, \"n\" for no).\n",
    "    display_dtype (str): A flag indicating whether to display dtype for each column or not (\"y\" for yes, \"n\" for no).\n",
    "    display_statistics (str): A flag indicating whether to display summary statitsics or not (\"y\" for yes, \"n\" for no).\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the entire dataset\n",
    "    print(\"Dataset:\")\n",
    "    print(datasets)\n",
    "\n",
    "    # Get dataset columns and shape\n",
    "    dataset_columns = list(datasets.columns)\n",
    "    dataset_shape = datasets.shape\n",
    "\n",
    "    # Calculate missing data statistics\n",
    "    missing_data_per_column = datasets.isnull().sum()\n",
    "    total_cells = np.product(dataset_shape)\n",
    "    total_missing = missing_data_per_column.sum()\n",
    "    percent_missing = (total_missing / total_cells) * 100\n",
    "\n",
    "    # Display dataset shape and missing data statistics\n",
    "    print(\"------------------------------------\")\n",
    "    print(f\"Dataset Shape: {dataset_shape}\")\n",
    "    print(f\"Total Cells: {total_cells}\")\n",
    "    print(f\"Total Missing: {total_missing}\")\n",
    "    print(f\"Percentage of Missing Data: {percent_missing:.2f}%\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    # Display columns with missing values and their counts\n",
    "    print(\"Columns with Missing Values:\")\n",
    "    for column in dataset_columns:\n",
    "        if datasets[column].isnull().sum() > 0:\n",
    "            print(f\" {column}, Missing Values: {datasets[column].isnull().sum()}\")\n",
    "\n",
    "    # Count the occurrence of each data type\n",
    "    dataset_datatypes = {}\n",
    "    for column in dataset_columns:\n",
    "        data_type = datasets[column].dtype\n",
    "        if data_type in dataset_datatypes:\n",
    "            dataset_datatypes[data_type] += 1\n",
    "        else:\n",
    "            dataset_datatypes[data_type] = 1\n",
    "\n",
    "    # Display dataset datatypes with their counts\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Dataset Datatypes with Counts:\")\n",
    "    for data_type, count in dataset_datatypes.items():\n",
    "        print(f\" {data_type}: {count}\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    if display_columns == \"y\":\n",
    "        # Display dataset columns\n",
    "        print(\"Dataset Columns:\")\n",
    "        for column in dataset_columns:\n",
    "            print(column)\n",
    "    if display_dtype == \"y\":\n",
    "        # Display dataset columns\n",
    "        print(\"Dataset Columns Data Types:\")\n",
    "        print(datasets.info())\n",
    "    if display_statistics == \"y\":\n",
    "        # Display dataset columns\n",
    "        print(\"Dataset Summary Statistics:\")\n",
    "        print(train.describe().T)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "train_filepath = \"C:/Users/marti/Desktop/projects/House-Prices--Advanced-Regression-Techniques/train.csv\"\n",
    "test_filepath = \"C:/Users/marti/Desktop/projects/House-Prices--Advanced-Regression-Techniques/test.csv\"\n",
    "train= pd.read_csv(train_filepath)\n",
    "test= pd.read_csv(test_filepath)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"INITIAL STATE OF THE DATASET\")\n",
    "print(\"-\"*50)\n",
    "print(dataset_summary(train, display_columns=\"n\", display_dtype=\"y\", display_statistics=\"y\"))\n",
    "\n",
    "\n",
    "# Drop the 'Id' column and modify the DataFrame in place\n",
    "train.drop(columns=['Id'], inplace=True)\n",
    "\n",
    "# Drop duplicate rows from the DataFrame in place\n",
    "train.drop_duplicates(inplace=True)\n",
    "\n",
    "train['LotFrontage'].fillna(0, inplace=True)\n",
    "train['MasVnrArea'].fillna(0, inplace=True)\n",
    "train['GarageYrBlt'].fillna(0, inplace=True)\n",
    "\n",
    "columns_to_original_encode = ['GarageType', 'LotShape', 'LandContour', \n",
    "                              'LandSlope', 'ExterQual', 'ExterCond', \n",
    "                              'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
    "                              'BsmtFinType1', 'BsmtFinType2', 'HeatingQC',\n",
    "                              'KitchenQual', 'FireplaceQu', 'GarageFinish',\n",
    "                              'GarageQual', 'GarageCond', 'PoolQC',\n",
    "                              'Fence']\n",
    "\n",
    "# Initialize the ordinal encoder\n",
    "encoder = OrdinalEncoder()\n",
    "for column in columns_to_original_encode:\n",
    "\n",
    "    # Handle missing values by replacing with \"NA\" and then encoding\n",
    "    train[column].fillna(\"NA\", inplace=True)\n",
    "    train[column] = encoder.fit_transform(train[[column]])\n",
    "\n",
    "columns_to_frequency_encode = ['MSZoning', 'Utilities', 'LotConfig',\n",
    "                               'Neighborhood', 'Condition1', 'Condition2',\n",
    "                               'BldgType', 'HouseStyle', 'RoofStyle',\n",
    "                               'RoofMatl', 'Exterior1st', 'Exterior2nd',\n",
    "                               'MasVnrType', 'Foundation', 'Heating',\n",
    "                               'Electrical', 'Functional', 'MiscFeature',\n",
    "                               'SaleType', 'SaleCondition']\n",
    "\n",
    "for column in columns_to_frequency_encode:\n",
    "    train[column].fillna(\"NA\", inplace=True)\n",
    "\n",
    "    # Calculate frequency of each category in the current column\n",
    "    category_frequencies = train[column].value_counts(normalize=True)\n",
    "\n",
    "    # Replace categories with their frequencies\n",
    "    train[column] = train[column].map(category_frequencies)\n",
    "\n",
    "\n",
    "columns_to_one_hot_encode = ['Street', 'Alley', 'PavedDrive', 'CentralAir']\n",
    "\n",
    "# Create an empty DataFrame to store the one-hot encoded columns\n",
    "one_hot_encoded_df = pd.DataFrame()\n",
    "for column in columns_to_one_hot_encode:\n",
    "    train[column].fillna(\"NA\", inplace=True)\n",
    "\n",
    "    # Apply one-hot encoding to the current column\n",
    "    one_hot_encoded_column = pd.get_dummies(train[column], prefix=column, prefix_sep='_')\n",
    "\n",
    "    # Concatenate the one-hot encoded column with the new DataFrame\n",
    "    one_hot_encoded_df = pd.concat([one_hot_encoded_df, one_hot_encoded_column], axis=1)\n",
    "\n",
    "# Convert boolean values to integers (0s and 1s)\n",
    "one_hot_encoded_df = one_hot_encoded_df.astype(int)\n",
    "\n",
    "# Concatenate the one-hot encoded DataFrame with the original DataFrame\n",
    "train = pd.concat([train, one_hot_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original columns that were one-hot encoded\n",
    "train.drop(columns=columns_to_one_hot_encode, inplace=True)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"STATE OF THE DATASET AFTER CATEGORICAL ENCODING AND HANDLING MISSING VALUES\")\n",
    "print(\"-\"*50)\n",
    "print(dataset_summary(train, display_columns=\"n\", display_dtype=\"y\", display_statistics=\"y\"))\n",
    "\n",
    "#subset containig all original numerical features plus those who then went through original and freuqency encoding\n",
    "df_num = ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', \n",
    "          'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n",
    "          '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', \n",
    "          'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', \n",
    "          'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n",
    "          '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold','GarageType', \n",
    "          'LotShape', 'LandContour', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n",
    "          'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'FireplaceQu', \n",
    "          'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC','Fence', 'MSZoning', 'Utilities', 'LotConfig',\n",
    "          'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle',\n",
    "          'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating',\n",
    "          'Electrical', 'Functional', 'MiscFeature', 'SaleType', 'SaleCondition', 'SalePrice']\n",
    "subset_df = train.loc[:,df_num]\n",
    "\n",
    "\n",
    "# Display descriptive statistics for the 'SalePrice' column\n",
    "print(\"Descriptive statistics for the 'SalePrice' column\")\n",
    "print(train['SalePrice'].describe())\n",
    "\n",
    "# Plot a histogram for the 'SalePrice' column using seaborn\n",
    "sns.distplot(train['SalePrice'], hist_kws={'alpha': 0.4}, bins=100, color='r')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the correlation matrix for numerical features\n",
    "print(\"Train Correlation\")\n",
    "print(subset_df.corr())\n",
    "\n",
    "# Plot a heatmap of the correlation matrix\n",
    "cormat = subset_df.corr()\n",
    "paper = plt.figure(figsize=(7, 8))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(cormat, cmap=\"coolwarm\", cbar=True, vmax=1, square=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histograms for numerical features\n",
    "#subset_df.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8, color='Red')\n",
    "#plt.show()\n",
    "\n",
    "# Select the best numerical features based on correlation with 'SalePrice'\n",
    "subset_df_corr = subset_df.corr()['SalePrice'][:-1]\n",
    "best_num_features = subset_df_corr[abs(subset_df_corr) > 0.4].sort_values(ascending=False)\n",
    "print(f\"There are {len(best_num_features)} best features with SalePrice:\\n\\n{best_num_features}\")\n",
    "\n",
    "# Calculate the correlation matrix for numerical features\n",
    "correlation_matrix = subset_df.corr()\n",
    "sales_price_corr = correlation_matrix[\"SalePrice\"].sort_values(ascending=False)\n",
    "sale_price_corr_df = pd.DataFrame(sales_price_corr)\n",
    "\n",
    "# Plot a heatmap of correlation with SalePrice\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sale_price_corr_df, annot=True, cmap=\"coolwarm\", cbar=True)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Correlation Heatmap with SalePrice')\n",
    "plt.show()\n",
    "\n",
    "# Initialize a StandardScaler to standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the numerical features in the DataFrame 'df_num'\n",
    "df_num_scaled = scaler.fit_transform(subset_df)\n",
    "\n",
    "# Define the target column for which we will calculate mutual information\n",
    "target_column = 'SalePrice'\n",
    "\n",
    "# Calculate mutual information between standardized numerical features and the target\n",
    "mi = mutual_info_regression(df_num_scaled, subset_df[target_column])\n",
    "\n",
    "# Create a Series to store mutual information with feature names as indices\n",
    "mi_series = pd.Series(mi, index=subset_df.columns)\n",
    "\n",
    "# Print mutual information in descending order\n",
    "print(\"Mutual Information:\")\n",
    "print(mi_series.sort_values(ascending=False))\n",
    "\n",
    "\n",
    "features_train_A = ['OverallQual', 'GrLivArea' ,'GarageCars', 'TotalBsmtSF', 'GarageArea', \n",
    "                   'YearBuilt', 'Neighborhood','BsmtQual', 'KitchenQual', 'ExterQual' ,\n",
    "                   '1stFlrSF', 'MSSubClass', 'GarageFinish', 'FullBath', 'GarageYrBlt',\n",
    "                   'YearRemodAdd', 'TotRmsAbvGrd', 'SalePrice']\n",
    "train_A = train.loc[:, features_train_A]\n",
    "print()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "At this point, i have a dataframe containing the best features for both mutual information rate and correlation rate\n",
    "you have to calculate this also for cat variables that were frequency and original encoded, not oe-hot encoded\n",
    "then modify the best features for both mutual information rate and correlation rate\n",
    "\n",
    "you have to just structure the code better to make it more readable, \n",
    "then understand the remaining lines what asks you to plot\n",
    "\n",
    "then build 3 neural networks and more models \n",
    "\n",
    "one with the best features + onehot encoded variables\n",
    "one with the best features \n",
    "\n",
    "work with each single neural network and hyperparameter tune them\n",
    "\n",
    "once you're done, compare the results and submit the results of the best one, then prepare a pdf file and post it on your linkedin profile\n",
    "then send it to the Quant team and asks for suggestions\n",
    "\n",
    "then congrats, the project is done (you'll come back eventually to optimize it but you can be over with it)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
